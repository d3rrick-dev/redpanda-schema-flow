spring:
  kafka:
    bootstrap-servers: localhost:9092
    topics:
      users: users-topic
      dlq: users-topic-dlt
      processed: processed-users-topic

    listener:
      type: single

    properties:
      schema.registry.url: http://localhost:8081

    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: io.confluent.kafka.serializers.KafkaAvroSerializer
      properties:
        auto.register.schemas: true

    consumer:
      group-id: my-avro-group
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: io.confluent.kafka.serializers.KafkaAvroDeserializer
      properties:
        specific.avro.reader: true

    streams:
      application-id: user-processing-stream
      bootstrap-servers:  ${spring.kafka.bootstrap-servers}
      # This tells Streams how to handle data by default
      properties:
        # 1. How often to commit (default is 30,000ms / 30s)
        # Setting this to 100ms makes it feel "real-time"
        commit.interval.ms: 100
        # 2. Disable the record cache
        # If this is 0, every single event triggers an update immediately
        cache.max.bytes.buffering: 0
        default:
          key:
            serde: org.apache.kafka.common.serialization.Serdes$StringSerde
          value:
            serde: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
# Expose the Prometheus endpoint
management:
  endpoints:
    web:
      exposure:
        include: "health,info,prometheus,metrics"
  endpoint:
    prometheus:
      access: unrestricted
    health:
      show-details: always
#  metrics:
#    export:
#      prometheus:
#        enabled: true

#logging:
#  level:
#    org:
#      apache:
#        kafka: DEBUG
#      springframework:
#        kafka: DEBUG

